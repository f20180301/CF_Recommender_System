# -*- coding: utf-8 -*-
"""Userbased.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SrCoNIuYjqauVEOjxYj4KBfkyFMW140R
"""

import pandas as pd
import numpy as np
from numpy import nan
from scipy import spatial
from sklearn.preprocessing import StandardScaler
import argparse 
import time

import warnings
warnings.filterwarnings("ignore")

# Initialize the Parser 
parser = argparse.ArgumentParser(description ='RS_eval')

#getting arguments 
parser.add_argument('--input',default='test_user.txt', help = 'input ratings file')

parser.add_argument('--output',default='output.csv', help = 'save output to this file')
  

args = parser.parse_args() 

#basepath = "/content/drive/My Drive/Acads/IR/MovieLensData/"
#basepath = "/content/drive/MyDrive/IR/MovieLensData"
basepath=""
ratings = pd.read_csv(basepath +  "ratings.csv")    #imported the RATINGS (user->movie ratings).
movie = pd.read_csv( basepath + "movies.csv")       #imported the movieIds (user->movie ratings).
#print(movie)                                       #testPrints
movieids=movie.loc[:,"movieId"].values
# print(movieid)                                    #testPrints

#USER-ITEM MATRIX CONSTRUCTION

np.set_printoptions(threshold=1000)
y=np.zeros((610,9742))                                      #Numpy Array 610X9742 to store User-Item Matrix
y_mean=np.zeros((610,1))                                    #Numpy Array 610X1 to store User Rating Average (Bias)
y[:]=np.nan                                                 #setting initialisation with NaN
for i in range (1,611):                
                                                            #loop to construct User_Item matrix, from Pandas DataFrame
  x_id=ratings[ratings["userId"] == i]['movieId']
  x_rating=ratings[ratings["userId"] == i]['rating']        #extracting i'th user's User_Ratings and movieIds

  x_id=np.concatenate((x_id.apply(lambda x_id: np.where(movieids==x_id)).values),axis=None) #x_id preprocesssing map movie_Ids, to Column Number, Saving space efficiently
  x_rating=np.concatenate((x_rating.values),axis=None)      #x_ratings preprocessing
  
  x_rating=x_rating.reshape(len(x_id),1)                    #reshaping x_ratings, to Column Vector for using with Standard Scaler of sklearn
  sc=StandardScaler(with_std=False)                         #model for Scaling  Average only
  x_rating=sc.fit_transform(x_rating)                       #Fit and Transform to remove user Bias
  y_mean[i-1]=sc.mean_                                      #storing i'th user mean 
  
  x_rating=np.concatenate(x_rating)                         #Undoing the reshape instruction
  
  np.put(y[i-1],x_id, x_rating)                             #putting the ratings of i'th user in a Sparse User-Item MAtrix, based on indices(x_id) 

print("\nUser Item Matrix Created Successfully\nShape: " ,y.shape)#row(user),col(mov_Index)


#print(y) #testing
#print(y_mean)#user-rating matric

#print(type(y))
## one to one mapping is used for ActualMoveiId, and ColNo, and hence size of the DS reduced to store the User-Movie Vector(sparse matrix)"##

#PreProcessing, removing number of items rated by less than or equal to 5 users
count = 0
for i in range(y.shape[1]):
  
  num_of_ratings = np.count_nonzero(~np.isnan(y[:, i]))
  
  if num_of_ratings <= 5:
    y[:, i] = np.nan
    count  = count + 1
#     print("i: ", i , " num of ratings:", num_of_ratings)
# print("count: ", count)
time.sleep(1)
print("\nUser Item Matrix modified (remove <5) Successfully\nShape: " ,y.shape)#row(user),col(mov_Index)

# Random 10 users
# content of test_user.txt file
test_user_list = [100, 263, 26, 45, 511, 454, 303, 340, 590, 199]
# user_ratings = ratings[ratings["userId"].isin(test_user_list)]
# user_ratings.drop(columns=["timestamp"], inplace = True)
# user_ratings.to_csv("test_user.txt", index = False)

# removing these from trainsset
y[test_user_list]  = np.nan

# PEARSON COEFF CALCULATION, FIRST FILTERING TO GET INTESECTION, NON-NANS, AND THEN COSINE SIM.
# CALCULATES COSINE SIMILARITY/PEARSON COEFF. BETWEEN 2 VECTORS
np.set_printoptions(precision=20)
def cosine_sim(df1, df2):
    
    df1na = np.isnan(df1)                  #isnan check, to track NaNs
    df1clean = df1[~df1na]                 #non-NaNs filtered
    df2clean = df2[~df1na]                 #non-NaNs filtered
    df2na = np.isnan(df2clean)             #isnan check, to track NaNs
    df1clean = df1clean[~df2na]            #non-NaNs filtered
    df2clean = df2clean[~df2na]            #non-NaNs filtered
    #intersection of those elements that are rated by Both USers, df1, df2.
   
    # Compute cosine similarity
    distance = 1-spatial.distance.cosine(df1clean, df2clean)
    return distance #within [-1,1]

#CALCULATES COSINE SIMILARITY/PEARSON COEFF. BETWEEN ONE VECTOR AND THE ENTORE DATASET
def cos_sim(row_vector,train_set):
  similarity = np.empty(shape=(len(train_set), 1))        #empty vector to store i'th user's similarity with all others in Train_Set.
  for i  in range(len(train_set)):
    similarity[i]=cosine_sim(row_vector,train_set[i])     #CALLS COSINE_SIM(VECTOR,VECTOR)
  return similarity                                       #similarity vector returned

#TOP K NEIGHBOURS PREDICTIONS 
np.set_printoptions(precision=4)
def predict_ratings(test_user,train_set,k):
  row_vector=np.array(test_user, copy=True) 
  similarity=cos_sim(row_vector,train_set)    # Pearson correlation coefficient of row_vector(i'th user), with entire train_set

  # Looping over each item of the test_user. If it is not rated, then we select top k similar users who have rated the item and predict
  # the rating over that item.
  for i in range(len(row_vector)):
    if(np.isnan(row_vector[i]) == False):
      continue

    idx = np.argwhere(~np.isnan(train_set[:,i]) == True)
    idx = idx.reshape((1, len(idx)))

    sim=similarity.reshape((1,len(similarity)))
    selected_sim=(np.take(sim,idx))
    filtertrain =(np.take(train_set,idx,axis=0)) 
    selected_sim = np.transpose(selected_sim)

    indices = (-selected_sim).argsort(axis=0)[:k]
    indices=indices.reshape((1,len(indices)))
    
    selected_sim=selected_sim.reshape((1,len(selected_sim)))    #reshaping for further use
    selected_set=(np.take(filtertrain[0],indices,axis=0))       #filter k-top users
    selected_newsim=(np.take(selected_sim,indices))
    selected_set=selected_set[0][:,i]

    # resnick prediction formula ( without adding the mean of the user)
    sum1 =np.sum(np.absolute(selected_newsim))    
    numerator = np.dot(selected_newsim,selected_set)
    row_vector[i] = numerator/sum1

  return row_vector   # predicted rating with user bias ( not added the average of the user)

np.set_printoptions(threshold = np.inf)
# print(y[0]+y_mean[0])
# print("nans ,",np.count_nonzero(np.isnan(y[0])))
# print("non-nans ,",len(y[0]) - np.count_nonzero(np.isnan(y[0])))

# vec = predict_ratings(y[2], y[11:], 40)
vec = predict_ratings(y[0], y[1:], 40)
sol=vec+y_mean[0]
sol[sol > 5] = 5
sol[sol < 1] = 1
sol[sol < 0] = np.nan

# print(sol)
# print("Num of greater than 5: ", np.count_nonzero(np.where(sol > 5)))
# print("---------", i)
# print("max is: ", np.nanmax(sol))
# print("min is: ", np.nanmin(sol))
# # print(np.argwhere(sol == np.nanmax(sol)))
# print(np.count_nonzero(np.isnan(sol)))

# print("1st user",y_mean[0])

# output dataframe created here
output_df = pd.DataFrame()
output_df["userno"] =[]
output_df["Moviepred"] =[]
output_df["predratings"]=[]
output_df["movieold"] =[]
output_df["movieoldratings"]=[]

# predictions on user
def output_util(userno, user, vec):
  global output_df
  predicted_indices = np.nonzero(~np.isnan(user))
  vec[predicted_indices] = np.nan
  indices = (-vec).argsort(axis = 0)[:5]
  past_movie_indices = (-user).argsort(axis = 0)[:5]
  
  vec[vec > 5] = 5
  vec[vec < 1] = 1
  vec[vec < 0] = np.nan
  #movieids[indices[i]]
  # movieids[past_movie_indices[i]]
  for i in range(len(indices)):
    df2 = {'userno': test_user_list[userno - 1], 'Moviepred': movie["title"][indices[i]], 'predratings': vec[indices[i]], 'movieold':movie["title"][past_movie_indices[i]], 'movieoldratings' : user[past_movie_indices[i]] }
    output_df = output_df.append(df2, ignore_index = True)
    #print("userno: ", test_user_list[userno], "i: ",  i)

#filename is test_user.txt
#REMEMBER TRAINSET AND K HAVE TO HARDCODED.
np.set_printoptions(precision=4)
def output(filename, trainset, k):

  user_df = pd.read_csv(filename)
  user=np.zeros((10,9742))                                      #Numpy Array 610X9742 to store User-Item Matrix
  user_mean=np.zeros((10,1))                                    #Numpy Array 610X1 to store User Rating Average (Bias)
  user[:]=np.nan                                                 #setting initialisation with NaN

  for i in range (1,11):                
                                                              #loop to construct User_Item matrix, from Pandas DataFrame
    x_id=user_df[user_df["userId"] == test_user_list[i -1]]['movieId']
    x_rating=user_df[user_df["userId"] == test_user_list[i - 1]]['rating']        #extracting i'th user's User_Ratings and movieIds

    x_id=np.concatenate((x_id.apply(lambda x_id: np.where(movieids==x_id)).values),axis=None) #x_id preprocesssing map movie_Ids, to Column Number, Saving space efficientky
    x_rating=np.concatenate((x_rating.values),axis=None)      #x_ratings preprocessing
    
    x_rating=x_rating.reshape(len(x_id),1)                    #reshaping x_ratings, to Column Vector for using with Standard Scaler of sklearn
    sc=StandardScaler(with_std=False)                         #model for Scaling  Average only
    x_rating=sc.fit_transform(x_rating)                       #Fit and Transform to remove user Bias
    user_mean[i-1]=sc.mean_                                      #storing i'th user mean 
    
    x_rating=np.concatenate(x_rating)                         #Undoing the reshape instruction
    
    np.put(user[i-1],x_id, x_rating)                             #putting the ratings of i'th user in a Sparse User-Item MAtrix, based on indices(x_id) 
    
  #print(y) #testing
  #print(y_mean)#user-rating matric
  print("\nTest User Item Matrix Created Successfully\nShape: " ,user.shape)#row(user),col(mov_Index)
  time.sleep(3.5)
  print("\nPredicting . . . \n")
  for i in range(1, 11):
    
    vec=predict_ratings(user[i - 1],trainset,k)
    sol = vec + user_mean[i - 1]
    
    output_util(i, user[i - 1] + user_mean[i - 1], sol)

# print(output_df)
output(args.input, y, 40)  # top k is 40
output_df.to_csv(args.output)

print("Successfully predicted top-5 movies for test users.")

# output_df.to_csv(args.output)