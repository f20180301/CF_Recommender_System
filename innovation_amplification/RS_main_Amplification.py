# -*- coding: utf-8 -*-
"""RS_main_Amplification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PVwiQI8oj-z5X-SYx5PvCMzPRF8EFkYk
"""

import pandas as pd
import numpy as np
from numpy import nan
from scipy import spatial
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold
import argparse

import warnings
warnings.filterwarnings("ignore")

# Initialize the Parser 
parser = argparse.ArgumentParser(description ='RS_eval')

#getting arguments 
parser.add_argument('--input',default='ratings.csv', help = 'input ratings file')

parser.add_argument('--output',default='eval_3.csv', help = 'save output to this file')
  

args = parser.parse_args()

#basepath = "/content/drive/MyDrive/Acads/IR/MovieLensData/"
#basepath = "/content/drive/MyDrive/IR/MovieLensData"
basepath=""
ratings = pd.read_csv(basepath +  args.input)    #imported the RATINGS (user->movie ratings).
movie = pd.read_csv( basepath + "movies.csv")       #imported the movieIds (user->movie ratings).
#print(movie)                                       #testPrints
movieids=movie.loc[:,"movieId"].values
# print(movieid)                                    #testPrints

#USER-ITEM MATRIX CONSTRUCTION

np.set_printoptions(threshold=1000)
y=np.zeros((610,9742))                                      #Numpy Array 610X9742 to store User-Item Matrix
y_mean=np.zeros((610,1))                                    #Numpy Array 610X1 to store User Rating Average (Bias)
y[:]=np.nan                                                 #setting initialisation with NaN
for i in range (1,611):                
                                                            #loop to construct User_Item matrix, from Pandas DataFrame
  x_id=ratings[ratings["userId"] == i]['movieId']
  x_rating=ratings[ratings["userId"] == i]['rating']        #extracting i'th user's User_Ratings and movieIds

  x_id=np.concatenate((x_id.apply(lambda x_id: np.where(movieids==x_id)).values),axis=None) #x_id preprocesssing map movie_Ids, to Column Number, Saving space efficiently
  x_rating=np.concatenate((x_rating.values),axis=None)      #x_ratings preprocessing
  
  x_rating=x_rating.reshape(len(x_id),1)                    #reshaping x_ratings, to Column Vector for using with Standard Scaler of sklearn
  sc=StandardScaler(with_std=False)                         #model for Scaling  Average only
  x_rating=sc.fit_transform(x_rating)                       #Fit and Transform to remove user Bias
  y_mean[i-1]=sc.mean_                                      #storing i'th user mean 
  
  x_rating=np.concatenate(x_rating)                         #Undoing the reshape instruction
  
  np.put(y[i-1],x_id, x_rating)                             #putting the ratings of i'th user in a Sparse User-Item MAtrix, based on indices(x_id) 

print("\nUser Item Matrix Created Successfully\nShape: " ,y.shape)#row(user),col(mov_Index)

#PreProcessing, removing number of items rated by less than or equal to 5 users
count = 0
for i in range(y.shape[1]):
  
  num_of_ratings = np.count_nonzero(~np.isnan(y[:, i]))
  
  if num_of_ratings <= 5:
    y[:, i] = np.nan
    count  = count + 1

# PEARSON COEFF CALCULATION, FIRST FILTERING TO GET INTESECTION, NON-NANS, AND THEN COSINE SIM.
# CALCULATES COSINE SIMILARITY/PEARSON COEFF. BETWEEN 2 VECTORS
np.set_printoptions(precision=6)
def cosine_sim(df1, df2, flag):
    
    df1na = np.isnan(df1)                  #isnan check, to track NaNs
    df1clean = df1[~df1na]                 #non-NaNs filtered
    df2clean = df2[~df1na]                 #non-NaNs filtered
    df2na = np.isnan(df2clean)             #isnan check, to track NaNs
    df1clean = df1clean[~df2na]            #non-NaNs filtered
    df2clean = df2clean[~df2na]            #non-NaNs filtered
    #intersection of those elements that are rated by Both USers, df1, df2.
   
    # Compute cosine similarity
    distance = 1-spatial.distance.cosine(df1clean, df2clean)

    # case amplification weighting innovation
    if flag == 1:
      if distance < 0:
        distance = -(abs(distance)**(2.5))
      else:
        distance = distance**(2.5)

    # significance weighting innovation
    if flag == 2:
      distance = len(df1clean)*distance

    return distance #within [-1,1]

#CALCULATES COSINE SIMILARITY/PEARSON COEFF. BETWEEN ONE VECTOR AND THE ENTORE DATASET
def cos_sim(row_vector,train_set, flag):
  similarity = np.empty(shape=(len(train_set), 1))        #empty vector to store i'th user's similarity with all others in Train_Set.
  for i  in range(len(train_set)):
    similarity[i]=cosine_sim(row_vector,train_set[i], flag)     #CALLS COSINE_SIM(VECTOR,VECTOR)
  return similarity                                       #similarity vector returned

#TOP K NEIGHBOURS PREDICTIONS 
np.set_printoptions(precision=4)
def predict_ratings_v2(test_user,movie_ids,train_set,k, flag):
  row_vector=np.array(test_user, copy=True) 
  similarity=cos_sim(row_vector,train_set, flag);    # Pearson correlation coefficient of row_vector(i'th user), with entire train_set

  # Looping over each item of the test_user. If it is not rated, then we select top k similar users who have rated the item and predict
  # the rating over that item.
  for i in movie_ids:
    # if(np.isnan(row_vector[i]) == False):
    #   continue

    idx = np.argwhere(~np.isnan(train_set[:,i]) == True)
    idx = idx.reshape((1, len(idx)))

    sim=similarity.reshape((1,len(similarity)))
    selected_sim=(np.take(sim,idx))
    filtertrain =(np.take(train_set,idx,axis=0)) 
    selected_sim = np.transpose(selected_sim)

    indices = (-selected_sim).argsort(axis=0)[:k]
    indices=indices.reshape((1,len(indices)))
    
    selected_sim=selected_sim.reshape((1,len(selected_sim)))    #reshaping for further use
    selected_set=(np.take(filtertrain[0],indices,axis=0))       #filter k-top users
    selected_newsim=(np.take(selected_sim,indices))
    selected_set=selected_set[0][:,i]

    # resnick prediction formula ( without adding the mean of the user)
    sum1 =np.sum(np.absolute(selected_newsim))    
    numerator = np.dot(selected_newsim,selected_set)
    row_vector[i] = numerator/sum1
  
  return row_vector   # predicted rating with user bias ( not added the average of the user)

#Function to calculate mae values
def get_mae(y_true, y_pred, movie_ids):
  mae = 0
  if len(movie_ids) == 0:
    return -1

  for i in movie_ids:
    if np.isnan(y_pred[i]):
      continue
    # print("y_true: ", y_true)
    # print("y_pred: ", y_pred)
    mae = mae + abs(y_true[i] - y_pred[i])
  mae = mae / len(movie_ids)
# print(mae)
  return mae

# Input y is user matrix

def cross_val_mae(y, topk, flag):
  # take user-item matrix
  # get list of non_nan (user_id,movie_id) pairs
  fold=0
  ratings = np.argwhere(~np.isnan(y))
  mae_scores = np.zeros((5,y.shape[0]))
  mae_final = np.zeros(5)

  # breaks into 5 parts (cross_val)
  cv = KFold(n_splits = 5, shuffle = True, random_state = 0)

  for train_index, test_index in cv.split(ratings):
    print("\nWorking on fold no.", fold+1, "\n")
    ui_copy = np.array(y, copy = True)
    
    # makes copy of user-item matrix and makes all test-item values as NaN and remembers the indices
    ui_copy[ratings[test_index][0],ratings[test_index][1]] = np.nan
    
    # running loop to take each user
    for i in range(y.shape[0]):

      movie_ids = ratings[test_index,1][ratings[test_index,0] == i]
      
      
      y_pred = predict_ratings_v2(ui_copy[i],movie_ids, ui_copy ,topk, flag)
      y_pred = y_pred + y_mean[i]
      y_pred[y_pred > 5] = 5
      y_pred[y_pred < 0] = np.nan
      y_pred[y_pred < 1] = 1

      mae_scores[fold][i] = get_mae(y[i] + y_mean[i],y_pred, movie_ids)
    # print(mae_scores[fold])  
    mae_final[fold] = np.nanmean(mae_scores[fold])
    print("foldmae: ", mae_final[fold])
    print("Fold ", fold+1, " training completed\n")


    fold = fold + 1   


  return mae_final

# Flag is sent into mae_values
# Flag = 0 : No improvement applied
# Flag = 1: Impovement of Case Amplification
# Flag = 2: Improvement of Significance weighting
# mae_values = cross_val_mae(y, 20, 0) 
mae_values = cross_val_mae(y, 20, 1)
# mae_values_imp2 = cross_val_mae(y, 20, 2)

eval = pd.DataFrame()
eval["Fold_number"]=[]
eval["MAE_Values"]=[]
for i in range(5):
  df2 = {'Fold_number' : i+1, 'MAE_Values': mae_values[i]}
  eval = eval.append(df2, ignore_index = True)
eval.to_csv(args.output)